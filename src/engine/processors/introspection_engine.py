import os
import json
from openai import OpenAI
from typing import List, Tuple, Dict, Any, Optional
from src.engine.models.entry import Entry, EntryType

class IntrospectionEngine:
    """
    A cognitive processor that reflects on new information in the context of
    existing memories to generate new insights, questions, or identify paradoxes
    by leveraging a Large Language Model.
    """
    def __init__(self):
        # API-ключ должен быть установлен в переменной окружения OPENAI_API_KEY
        # Вы можете создать файл .env в корне проекта и записать в него:
        # OPENAI_API_KEY="sk-..."
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("WARNING: OPENAI_API_KEY environment variable not set. IntrospectionEngine will be disabled.")
            self.client = None
        else:
            self.client = OpenAI(api_key=api_key)
        print("IntrospectionEngine initialized.")

    def _create_prompt(self, original_text: str, associated_content: str) -> str:
        return f"""
You are a metacognitive introspection engine.
Your task is to analyze a new thought and its strongest associated memory.
Based on this, you must determine if a new insight, paradox, or question arises.

New Thought: "{original_text}"
Associated Memory: "{associated_content}"

Analyze these two statements.
- If they are trivial, only rephrase the same idea, or the connection is weak, respond with: {{"action": "none"}}
- If they reveal a deeper connection or a new layer of understanding, respond with: {{"action": "create_insight", "content": "A concise new insight here."}}
- If they contradict each other, respond with: {{"action": "create_paradox", "content": "A concise statement of the paradox here."}}
- If the new thought raises a new question based on the memory, respond with: {{"action": "create_question", "content": "A concise new question here."}}

Your response MUST be a single, valid JSON object and nothing else.
"""

    def analyze(self, original_text: str, associations: List[Tuple[Dict[str, Any], float]]) -> Optional[Entry]:
        if not self.client or not associations:
            return None

        strongest_association, _ = associations[0]
        associated_content = strongest_association.get('content', '')
        
        prompt = self._create_prompt(original_text, associated_content)
        
        try:
            print("\n[IntrospectionEngine] Querying LLM for insights...")
            response = self.client.chat.completions.create(
                model="gpt-4o-mini", # Экономичная и быстрая модель
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.2, # Низкая температура для более предсказуемого вывода
            )
            
            result_json = response.choices[0].message.content
            result = json.loads(result_json)
            
            action = result.get("action")
            content = result.get("content")

            print(f"[IntrospectionEngine] LLM Action: {action}")

            if not content:
                return None
            
            entry_map = {
                "create_insight": EntryType.INSIGHT,
                "create_paradox": EntryType.PARADOX,
                "create_question": EntryType.QUESTION,
            }

            if action in entry_map:
                return Entry(
                    entry_type=entry_map[action],
                    content=content,
                    context=f"Generated by LLM based on thought: '{original_text}'"
                )

        except Exception as e:
            print(f"ERROR: An error occurred while querying OpenAI: {e}")
        
        return None 